{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mm71i5YYYJNP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import imageio\n",
        "from tqdm.notebook import trange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\",map_name=\"4x4\",is_slippery=False)"
      ],
      "metadata": {
        "id": "HQaal4FoYTqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ],
      "metadata": {
        "id": "QqmGAnsOZJr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable\n",
        "\n",
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ],
      "metadata": {
        "id": "8UhMklIUYYo0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  random_int = random.uniform(0,1)\n",
        "  if random_int > epsilon:\n",
        "    action = np.argmax(Qtable[state])\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "  return action"
      ],
      "metadata": {
        "id": "7Rk6ZsquYba2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000\n",
        "learning_rate = 0.7\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"\n",
        "max_steps = 99\n",
        "gamma = 0.95\n",
        "eval_seed = []\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.05\n",
        "decay_rate = 0.0005"
      ],
      "metadata": {
        "id": "1hWNiWG1Yfy9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in trange(n_training_episodes):\n",
        "\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "      # If done, finish the episode\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "      # Our state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ],
      "metadata": {
        "id": "qIuYFzLPYwvm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ],
      "metadata": {
        "id": "jHc-y3oeYymF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    if seed:\n",
        "      state = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # Take the action (index) that have the maximum reward\n",
        "      action = np.argmax(Q[state][:])\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "kjGuzeFDY2xB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "NT8k3tMdY4R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, Qtable, out_directory, duration=1):\n",
        "  images = []\n",
        "  done = False\n",
        "  state = env.reset(seed=random.randint(0,500))\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], duration=duration)"
      ],
      "metadata": {
        "id": "BZKJvob0Y69y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path=\"/content/replay.gif\"\n",
        "video_duration=3\n",
        "record_video(env, Qtable_frozenlake, video_path, video_duration)\n",
        "\n",
        "from IPython.display import Image\n",
        "Image('./replay.gif')"
      ],
      "metadata": {
        "id": "MFxcvBs3Y9Le"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}